{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AgentQnA vLLM Deployment & Evaluation Tutorial\n",
    "\n",
    "## Table of Contents\n",
    "1. [Overview](#overview)\n",
    "2. [Prerequisites](#prerequisites)\n",
    "3. [System Architecture](#system-architecture)\n",
    "4. [Deployment Guide](#deployment-guide)\n",
    "5. [Performance Evaluation](#performance-evaluation)\n",
    "6. [Monitoring](#monitoring)\n",
    "7. [Troubleshooting](#troubleshooting)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview <a id=\"overview\"></a>\n",
    "\n",
    "AgentQnA is a Retrieval-Augmented Generation (RAG) system built from three cooperative LangChain agents and a high-performance LLM served by **vLLM** on AMD GPUs (ROCm). This notebook walks you through single-node deployment and basic performance testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites <a id=\"prerequisites\"></a>\n",
    "\n",
    "• AMD MI300X (or other ROCm GPU) • Docker & Docker Compose\n",
    "• Python 3.12           • Hugging Face access token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy AgentQnA on AMD GPU (ROCm)\n",
    "\n",
    "This document outlines the single-node deployment process for an AgentQnA application utilizing the GenAIComps micro-services on an AMD GPU (ROCm) server. The steps include pulling Docker images, container deployment via Docker Compose, and service execution using the AgentQnA micro-services.\n",
    "\n",
    "## Background & High-Level Architecture\n",
    "\n",
    "AgentQnA is a multi-tool, retrieval-augmented reasoning agent derived from the **OPEA** project. It couples several specialised sub-agents (RAG, SQL and ReAct) with a language-model back-end (**vLLM** or **TGI**) to answer heterogeneous user questions with minimal hallucination.\n",
    "\n",
    "The end-to-end data flow is shown below.\n",
    "\n",
    "```text\n",
    "┌──────────────┐      ┌────────────────────┐     ┌────────────────────────┐\n",
    "│  User Query  │ ───► │  Supervisor Agent  │ ───► │  Tool Invocation / RAG │\n",
    "└──────────────┘      └─────────┬──────────┘     └────────────┬───────────┘\n",
    "                                │                             │\n",
    "                        ┌───────▼─────────┐     ┌─────────────▼─────────┐\n",
    "                        │  Worker RAG     │     │   Worker SQL Agent    │\n",
    "                        │  (LangChain)    │     │  (Table QA)           │\n",
    "                        └───────┬─────────┘     └─────────────┬─────────┘\n",
    "                                │                             │\n",
    "                         ┌──────▼─────┐               ┌───────▼─────┐\n",
    "                         │  Retriever │               │  Postgres   │\n",
    "                         │ (Redis-IVF)│               │  or SQLite  │\n",
    "                         └──────┬─────┘               └─────────────┘\n",
    "                                │\n",
    "                      ┌─────────▼─────────┐\n",
    "                      │    vLLM / TGI     │  (LLM inference on ROCm GPUs)\n",
    "                      └────────────────────┘\n",
    "```\n",
    "\n",
    "**Key points:**\n",
    "\n",
    "* **vLLM (ROCm 6.4.1)** provides fast GPT-style decoding with multi-GPU tensor-parallelism.\n",
    "* **Redis Vector** stores embeddings generated at ingestion time and serves similarity search during retrieval.\n",
    "* **Prometheus + Grafana** (optional) expose GPU utilisation, token throughput and cache hit-rates.\n",
    "* All services are orchestrated via **Docker Compose** and can be launched with one-liner helper scripts.\n",
    "\n",
    "The remainder of this tutorial focuses on deploying the vLLM-backed stack, validating the agent endpoints, and benchmarking performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Guide <a id=\"deployment-guide\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Clone repositories\n",
    "!git clone https://github.com/opea-project/GenAIExamples.git\n",
    "!git clone https://github.com/Yu-amd/LaunchPad.git  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Copy helper scripts from LaunchPad (identical tree layout)\n",
    "%cd LaunchPad/GenAIExamples/AgentQnA/docker_compose/amd/gpu/rocm\n",
    "!cp *.sh  /path/to/GenAIExamples/AgentQnA/docker_compose/amd/gpu/rocm/\n",
    "!cp *.yaml /path/to/GenAIExamples/AgentQnA/docker_compose/amd/gpu/rocm/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Configure environment\n",
    "%cd /path/to/GenAIExamples/AgentQnA/docker_compose/amd/gpu/rocm\n",
    "export AGENTQNA_HUGGINGFACEHUB_API_TOKEN=\"YOUR_HUGGING_FACE_TOKEN\"\n",
    "source set_env_vllm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option A – One-click script (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./run_agentqna.sh setup-vllm\n",
    "!./run_agentqna.sh start-vllm\n",
    "!./run_agentqna.sh status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option B – Manual Compose launch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source set_env_vllm.sh\n",
    "docker compose -f compose_vllm.yaml up -d\n",
    "docker compose ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List containers\n",
    "!docker ps --format \"table {{.Names}}\\t{{.Status}}\\t{{.Ports}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test against vLLM service\n",
    "!curl -X POST http://localhost:18009/v1/chat/completions \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"model\":\"${AGENTQNA_LLM_MODEL_ID}\",\"messages\":[{\"role\":\"user\",\"content\":\"Hello!\"}]}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation <a id=\"performance-evaluation\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./quick_eval_setup.sh   # installs GenAIEval + deps\n",
    "!./performance_evaluation.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring <a id=\"monitoring\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./start_monitoring.sh   # Prometheus + Grafana stack\n",
    "echo \"Grafana ⇒ http://<host_ip>:3000  (admin / admin)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting <a id=\"troubleshooting\"></a>\n",
    "\n",
    "• **Service not starting** – `docker compose logs -f`\n",
    "• **GPU OOM**        – lower `--max-model-len` in [compose_vllm.yaml](cci:7://file:///root/ethanliu/LaunchPad/GenAIExamples/AgentQnA/docker_compose/amd/gpu/rocm/compose_vllm.yaml:0:0-0:0)\n",
    "• **Slow responses**   – monitor GPU util in `rocm-smi` and adjust batch size\n",
    "• **Restart stack**   – `./run_agentqna.sh restart-vllm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "© 2025 Advanced Micro Devices, Inc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
