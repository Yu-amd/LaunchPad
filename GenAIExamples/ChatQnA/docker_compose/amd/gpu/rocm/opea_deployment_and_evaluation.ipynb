{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatQnA vLLM Deployment and Performance Evaluation Tutorial\n",
    "\n",
    "## Table of Contents\n",
    "1. [Overview](#overview)\n",
    "2. [Prerequisites](#prerequisites)\n",
    "3. [System Architecture](#system-architecture)\n",
    "4. [Deployment Guide](#deployment-guide)\n",
    "5. [Performance Evaluation](#performance-evaluation)\n",
    "6. [Monitoring and Troubleshooting](#monitoring-and-troubleshooting)\n",
    "7. [Advanced Configuration](#advanced-configuration)\n",
    "8. [Troubleshooting](#troubleshooting)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "ChatQnA is a Retrieval-Augmented Generation (RAG) system that combines document retrieval with LLM inference. This tutorial provides a comprehensive guide for deploying ChatQnA using vLLM on AMD GPUs with ROCm support, and performing pipeline performance evaluation.\n",
    "\n",
    "### Key Features\n",
    "- **vLLM Integration**: LLM serving with optimized inference on AMD Instinct GPUs\n",
    "- **AMD GPU Support**: ROCm-based GPU acceleration\n",
    "- **Vector Search**: Redis-based document retrieval\n",
    "- **RAG Pipeline**: Complete question-answering system\n",
    "- **Performance Monitoring**: Built-in metrics and evaluation tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- **AMD Developer Cloud**: 1xMI300X GPU / 192 GB VRAM / 20 vCPU / 240 GB RAM Droplet\n",
    "- **Hugging Face Token**: For model access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Architecture\n",
    "\n",
    "### Service Components\n",
    "\n",
    "The following is the complete system architecture diagram.\n",
    "\n",
    "**Architecture Overview:**\n",
    "```\n",
    "┌───────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                               EXTERNAL ACCESS                                     │\n",
    "│                                                                                   │\n",
    "│   ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────────────────┐   │\n",
    "│   │   Web Browser   │    │   API Clients   │    │      Monitoring Tools       │   │\n",
    "│   │                 │    │                 │    │    (Grafana, Prometheus)    │   │\n",
    "│   └─────────────────┘    └─────────────────┘    └─────────────────────────────┘   │\n",
    "│           │                       │                           │                   │\n",
    "│           │                       │                           │                   │\n",
    "│           ▼                       ▼                           ▼                   │\n",
    "│   ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────────────────┐   │\n",
    "│   │   Nginx Proxy   │    │   Backend API   │    │        Redis Insight        │   │\n",
    "│   │   (Port 8081)   │    │   (Port 8890)   │    │         (Port 8002)         │   │\n",
    "│   └─────────────────┘    └─────────────────┘    └─────────────────────────────┘   │\n",
    "│           │                       │                           │                   │\n",
    "│           │                       │                           │                   │\n",
    "│           ▼                       ▼                           ▼                   │\n",
    "│   ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────────────────┐   │\n",
    "│   │   Frontend UI   │    │     Backend     │    │   Redis Vector Database     │   │\n",
    "│   │   (Port 5174)   │    │     Server      │    │         (Port 6380)         │   │\n",
    "│   │   (React App)   │    │    (FastAPI)    │    │      (Vector Storage)       │   │\n",
    "│   └─────────────────┘    └─────────────────┘    └─────────────────────────────┘   │\n",
    "│                                   │                           │                   │\n",
    "│                                   │                           │                   │\n",
    "│                                   ▼                           ▼                   │\n",
    "│  ┌─────────────────────────────────────────────────────────────────────────────┐  │\n",
    "│  │                             RAG PIPELINE                                    │  │\n",
    "│  │                                                                             │  │\n",
    "│  │  ┌───────────────────┐ ┌─────────────────────┐ ┌─────────────────────────┐  │  │\n",
    "│  │  │ Retriever Service │ │TEI Embedding Service│ │  TEI Reranking Service  │  │  │\n",
    "│  │  │                   │ │                     │ │                         │  │  │\n",
    "│  │  │   (Port 7001)     │ │    (Port 18091)     │ │      (Port 18809)       │  │  │\n",
    "│  │  │                   │ │                     │ │                         │  │  │\n",
    "│  │  │ • Vector Search   │ │ • Text Embedding    │ │ • Document Reranking    │  │  │\n",
    "│  │  │ • Similarity      │ │ • BGE Model         │ │ • Relevance Scoring     │  │  │\n",
    "│  │  │   Matching        │ │ • CPU Inference     │ │ • CPU Inference         │  │  │\n",
    "│  │  └───────────────────┘ └─────────────────────┘ └─────────────────────────┘  │  │\n",
    "│  │            │                      │                         │               │  │\n",
    "│  │            │                      │                         │               │  │\n",
    "│  │            ▼                      ▼                         ▼               │  │\n",
    "│  │  ┌───────────────────────────────────────────────────────────────────────┐  │  │\n",
    "│  │  │                           vLLM Service                                │  │  │\n",
    "│  │  │                           (Port 18009)                                │  │  │\n",
    "│  │  │                                                                       │  │  │\n",
    "│  │  │                  • High-Performance LLM Inference                     │  │  │\n",
    "│  │  │                  • AMD GPU Acceleration (ROCm)                        │  │  │\n",
    "│  │  │                  • Qwen2.5-7B-Instruct Model                          │  │  │\n",
    "│  │  │                  • Optimized for Throughput & Latency                 │  │  │\n",
    "│  │  │                  • Tensor Parallel Support                            │  │  │\n",
    "│  │  └───────────────────────────────────────────────────────────────────────┘  │  │\n",
    "│  └─────────────────────────────────────────────────────────────────────────────┘  │\n",
    "│                                      │                                            │\n",
    "│                                      │                                            │\n",
    "│                                      ▼                                            │\n",
    "│  ┌─────────────────────────────────────────────────────────────────────────────┐  │\n",
    "│  │                            DATA PIPELINE                                    │  │\n",
    "│  │                                                                             │  │\n",
    "│  │  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────────────┐  │  │\n",
    "│  │  │   Dataprep      │    │   Model Cache   │    │   Document Storage      │  │  │\n",
    "│  │  │   Service       │    │   (./data)      │    │   (Redis Vector DB)     │  │  │\n",
    "│  │  │   (Port 18104)  │    │                 │    │                         │  │  │\n",
    "│  │  │                 │    │ • Downloaded    │    │ • Vector Embeddings     │  │  │\n",
    "│  │  │ • Document      │    │   Models        │    │ • Metadata Index        │  │  │\n",
    "│  │  │   Processing    │    │ • Model Weights │    │ • Full-Text Search      │  │  │\n",
    "│  │  │ • Text          │    │ • Cache Storage │    │ • Similarity Search     │  │  │\n",
    "│  │  │   Extraction    │    │ • Shared Volume │    │ • Redis Stack           │  │  │\n",
    "│  │  └─────────────────┘    └─────────────────┘    └─────────────────────────┘  │  │\n",
    "│  └─────────────────────────────────────────────────────────────────────────────┘  │\n",
    "└───────────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "**Additional Services:**\n",
    "- **Dataprep Service** (Port 18104): Document processing and ingestion\n",
    "- **Redis Insight** (Port 8002): Database monitoring interface\n",
    "- **Model Cache** (./data): Shared volume for model storage\n",
    "\n",
    "### Data Flow\n",
    "1. **User Input**: Question submitted via frontend\n",
    "2. **Embedding**: Question converted to vector using TEI service\n",
    "3. **Retrieval**: Similar documents retrieved from Redis vector database\n",
    "4. **Reranking**: Retrieved documents reranked for relevance\n",
    "5. **LLM Inference**: vLLM generates answer using retrieved context\n",
    "6. **Response**: Answer returned to user via frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Guide\n",
    "\n",
    "### Step 1: Pull source code from GitHub\n",
    "First, we'll clone the Open Platform for Enterprise AI (OPEA) GenAIExamples repository, which contains the ChatQnA implementation and other AI examples needed for our deployment. Next, we'll clone the LaunchPad repository that provides one-click deployment scripts and configuration files specifically designed for the ChatQnA use case on AMD GPU environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Platform for Enterprise AI (OPEA)\n",
    "!git clone https://github.com/opea-project/GenAIExamples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One click deployment scripts for the use case\n",
    "!git clone https://github.com/Yu-amd/LaunchPad.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LaunchPad project uses the same hierarchy as OPEA project. You need to copy the scripts and yaml files from the directory:  to the corresponding directory in OPEA folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy necessary scripts and configuration files to the OPEA directory\n",
    "!cp /root/LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/*.sh /root/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n",
    "!cp /root/LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/*.yaml /root/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n",
    "!cp /root/LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/.env /root/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Environment Setup\n",
    "Now we need to navigate to the OPEA deployment directory where all the configuration files and scripts are located.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the OPEA deployment directory\n",
    "%cd /root/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to set up environment variable management. First, we install the python-dotenv package which allows us to load environment variables from a .env file. Then we import the necessary modules and load the environment variables from the .env file, which contains our configuration settings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install and load .env\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()  # Loads variables from .env file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to configure your Hugging Face API token, which is required to download the AI models used by the ChatQnA system. Make sure to replace 'your_token_here' with your actual Hugging Face token.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your Hugging Face token and environment\n",
    "import os\n",
    "os.environ['CHATQNA_HUGGINGFACEHUB_API_TOKEN'] = 'your_token_here'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll set up the vLLM environment using the provided script. This will configure all the necessary components for high-performance LLM inference on AMD GPUs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup vLLM environment\n",
    "!./run_chatqna.sh setup-vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the environment configured, we can now start the vLLM services. This will launch all the necessary containers and services for the ChatQnA system.\n",
    "### Step 3: Deploy the workload\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start vLLM services\n",
    "!./run_chatqna.sh start-vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the status of all running services to ensure everything started correctly and is functioning properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service status\n",
    "!./run_chatqna.sh status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll monitor the vLLM service logs for 60 seconds to verify that the service is starting up correctly and to check for any potential issues during initialization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check chatqna-vllm-service status\n",
    "!timeout 60 docker logs -f chatqna-vllm-service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm chatqna-vllm-service is ready, go straight to Step 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Memory Management\n",
    "\n",
    "Before proceeding to verify deployment, it's important to ensure your GPU memory is properly managed.\n",
    "\n",
    "#### Check GPU Memory Status\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current GPU memory usage\n",
    "# Expected output shows VRAM% and GPU% usage\n",
    "# If VRAM% is high (>80%) but GPU% is low, memory may be fragmented\n",
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear GPU Memory (If Needed)\n",
    "\n",
    "If you encounter GPU memory issues or high VRAM usage with low GPU utilization:\n",
    "\n",
    "**Option 1: Kill GPU Processes**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find processes using GPU\n",
    "!sudo fuser -v /dev/kfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill GPU-related processes\n",
    "!sudo pkill -f \"python|vllm|docker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Restart GPU Services**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart AMD GPU services\n",
    "!sudo systemctl restart amdgpu\n",
    "!sudo systemctl restart kfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3: System Reboot (Most Reliable)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If other methods don't work, reboot the system\n",
    "# Note: If you're on a remote server, wait approximately 30 seconds to 1 minute\n",
    "# before attempting to SSH back into the server\n",
    "!sudo reboot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After clearing GPU memory, verify it's free:\n",
    "# Check GPU memory is now available\n",
    "# Expected: VRAM% should be low (<20%) and GPU% should be 0%\n",
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Verify Deployment\n",
    "Let's verify that all Docker containers are running properly by checking their current status and port mappings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check running containers\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test the backend API to ensure it's responding correctly. This will send a simple test message to verify that the ChatQnA service is working properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backend API\n",
    "!curl -X POST http://localhost:8890/v1/chatqna \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"Hello, how are you?\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Upload Documents\n",
    "Let's create a sample document and upload it to the system. This will demonstrate how to ingest documents into the ChatQnA system for retrieval and question answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text file\n",
    "!echo \"Your document content here\" > document.txt\n",
    "\n",
    "# Upload the file\n",
    "!curl -X POST http://localhost:18104/v1/dataprep/ingest \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"files=@document.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's verify that the document was successfully uploaded and indexed by checking the contents of the Redis vector database.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the upload worked\n",
    "# Check if the document was indexed\n",
    "!curl -X POST http://localhost:18104/v1/dataprep/get \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"index_name\": \"rag-redis\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also upload multiple documents at once. Here's how to create and upload several documents simultaneously to build up your knowledge base.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple documents\n",
    "# Create multiple files\n",
    "!echo \"Document 1 content\" > doc1.txt\n",
    "!echo \"Document 2 content\" > doc2.txt\n",
    "\n",
    "# Upload multiple files\n",
    "!curl -X POST http://localhost:18104/v1/dataprep/ingest \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"files=@doc1.txt\" \\\n",
    "  -F \"files=@doc2.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "\n",
    "### Overview\n",
    "\n",
    "Performance evaluation helps you understand:\n",
    "- **Throughput**: Requests per second\n",
    "- **Latency**: Response time\n",
    "- **Accuracy**: Answer quality\n",
    "- **Resource Usage**: CPU, GPU, memory utilization\n",
    "\n",
    "### Step 1: Setup Evaluation Environment\n",
    "Now let's set up the evaluation environment. We'll navigate to the root directory and clone the GenAIEval repository, which contains the benchmarking tools we'll use to evaluate the ChatQnA system's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change directory to top of workspace\n",
    "%cd /root",
    "# Pull from OPEA GitHub so GenAIExamples and GenAIEval are in the same directory\n",
    "!git clone https://github.com/opea-project/GenAIEval.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll copy the ChatQnA-specific evaluation scripts from the LaunchPad directory to the GenAIEval benchmark folder so we can run performance tests on our deployed system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy chatqna scripts from the LaunchPad directory\n",
    "!cp /root/LaunchPad/GenAIEval/evals/benchmarks/* /root/GenAIEval/evals/benchmark/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's navigate to the GenAIEval directory where we'll set up and run our performance evaluation tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to evaluation directory\n",
    "%cd /root/GenAIEval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to install the Python virtual environment package to create an isolated environment for our evaluation tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependency\n",
    "!apt install python3.12-venv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll create a virtual environment for our evaluation tools and activate it to ensure we have a clean, isolated Python environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create virtual environment\n",
    "!python3 -m venv opea_eval_env\n",
    "!source opea_eval_env/bin/activate"
   ]
  },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's install all the required dependencies for the evaluation tools and set up the GenAIEval package in development mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install evaluation dependencies\n",
    "!pip install -r requirements.txt\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's navigate back to the ChatQnA deployment directory where we'll run the performance evaluation tests on our deployed system.\n",
    "### Step 2: Run Basic Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate back to GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n",
    "%cd /root/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run the vLLM evaluation script which will test the performance of our ChatQnA system, measuring metrics like throughput, latency, and response quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run vLLM evaluation\n",
    "!./run_chatqna.sh vllm-eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Performance Metrics\n",
    "\n",
    "#### Throughput Testing\n",
    "We'll install Apache Bench (ab), a tool that will help us perform load testing and measure the throughput of our ChatQnA API under various conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependency\n",
    "!apt install apache2-utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a test file with a complex question that will help us evaluate how well the system handles detailed, multi-part queries and generates comprehensive responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complex test file\n",
    "!echo '{\"messages\": \"Can you provide a detailed explanation of how neural networks work, including the concepts of forward propagation, backpropagation, and gradient descent? Also explain how these concepts relate to deep learning and why they are important for modern AI systems.\"}' > test_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll run a load test using Apache Bench to simulate 100 concurrent requests with 10 simultaneous connections, which will help us measure the system's throughput and performance under stress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test concurrent requests\n",
    "!ab -n 100 -c 10 -p test_data.json -T application/json \\\n",
    "  http://localhost:8890/v1/chatqna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latency Testing\n",
    "Let's create a detailed timing format file for curl that will help us measure various latency metrics including DNS lookup, connection time, and total response time for precise performance analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create curl-format.txt with the following content:\n",
    "     time_namelookup:  %{time_namelookup}\\n\n",
    "        time_connect:  %{time_connect}\\n\n",
    "     time_appconnect:  %{time_appconnect}\\n\n",
    "    time_pretransfer:  %{time_pretransfer}\\n\n",
    "       time_redirect:  %{time_redirect}\\n\n",
    "  time_starttransfer:  %{time_starttransfer}\\n\n",
    "                     ----------\\n\n",
    "          time_total:  %{time_total}\\n\n",
    "          http_code:  %{http_code}\\n\n",
    "       size_download:  %{size_download}\\n\n",
    "      speed_download:  %{speed_download}\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use curl with our detailed timing format to measure the precise response times for a single request, which will give us granular insights into each step of the request processing pipeline.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure response times\n",
    "!curl -w \"@curl-format.txt\" -X POST http://localhost:8890/v1/chatqna \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"What is machine learning?\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluation Results\n",
    "\n",
    "Evaluation results include:\n",
    "- **Response Time**: Average, median, 95th percentile\n",
    "- **Throughput**: Requests per second\n",
    "- **Accuracy**: Answer quality metrics\n",
    "- **Resource Usage**: CPU, GPU, memory consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Issues and Solutions\n",
    "\n",
    "#### Issue 1: GPU Memory Errors\n",
    "**Symptoms**: out-of-memory or similar errors\n",
    "\n",
    "**Solution**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce batch size in vLLM configuratioin\n",
    "# Edit compose_vllm.yaml, modify vLLM service command:\n",
    "--max-model-len 2048 --tensor-parallel-size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issue 2: Service Startup Failures\n",
    "**Symptoms**: Services fail to start or remain in \"starting\" state\n",
    "\n",
    "**Solution**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logs for specific errors\n",
    "!docker compose -f compose_vllm.yaml logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart services\n",
    "!./run_chatqna.sh restart-vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issue 3: Redis Index Issues\n",
    "**Symptoms**: Retrieval service fails to find documents\n",
    "\n",
    "**Solution**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Redis index\n",
    "./fix_redis_index.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate index manually\n",
    "!docker exec chatqna-redis-vector-db redis-cli FT.CREATE rag-redis ON HASH PREFIX 1 doc: SCHEMA content TEXT WEIGHT 1.0 distance NUMERIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Issue 4: Model Download Failures\n",
    "**Symptoms**: Services fail to download models\n",
    "\n",
    "**Solution**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check HF token\n",
    "!echo $CHATQNA_HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set token manually\n",
    "!export CHATQNA_HUGGINGFACEHUB_API_TOKEN=\"your_token_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Configuration\n",
    "\n",
    "### Custom Model Configuration\n",
    "\n",
    "Edit set_env_vllm.sh to use different models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change LLM model\n",
    "!export CHATQNA_LLM_MODEL_ID=\"Qwen/Qwen2.5-14B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change embedding model\n",
    "!export CHATQNA_EMBEDDING_MODEL_ID=\"BAAI/bge-large-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change reranking model\n",
    "!export CHATQNA_RERANK_MODEL_ID=\"BAAI/bge-reranker-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Diagnostic Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system resources\n",
    "!./detect_issues.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete system\n",
    "!./quick_test_chatqna.sh eval-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service health\n",
    "!docker compose -f compose_vllm.yaml ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all logs\n",
    "!docker compose -f compose_vllm.yaml logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow specific service logs\n",
    "!docker compose -f compose_vllm.yaml logs -f chatqna-vllm-service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for errors\n",
    "!docker compose -f compose_vllm.yaml logs | grep -i error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial provides a comprehensive guide for deploying ChatQnA with vLLM on AMD GPUs and performing detailed performance evaluation. The system offers:\n",
    "\n",
    "- **High Performance**: vLLM-optimized inference\n",
    "- **Scalability**: Docker-based microservices architecture\n",
    "- **Monitoring**: Built-in performance metrics\n",
    "- **Flexibility**: Configurable models and parameters\n",
    "\n",
    "For additional support or advanced configurations, refer to the project documentation or create issues in the repository.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Customize Models**: Experiment with different LLM and embedding models\n",
    "2. **Scale Deployment**: Add multiple GPU nodes for higher throughput\n",
    "3. **Optimize Performance**: Fine-tune vLLM parameters for your specific use case\n",
    "4. **Monitor Production**: Set up comprehensive monitoring for production deployments\n",
    "\n",
    "### Useful Commands Reference\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "*Note*: This tutorial assumes you have the necessary permissions and that all required software is installed. For production deployments, consider additional security measures and monitoring solutions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
