{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatQnA vLLM Deployment and Performance Evaluation Tutorial\n\n",
    "## Table of Contents\n",
    "1. [Overview](#overview)\n",
    "2. [Prerequisites](#prerequisites)\n",
    "3. [System Architecture](#system-architecture)\n",
    "4. [Deployment Guide](#deployment-guide)\n",
    "5. [Performance Evaluation](#performance-evaluation)\n",
    "6. [Monitoring and Troubleshooting](#monitoring-and-troubleshooting)\n",
    "7. [Advanced Configuration](#advanced-configuration)\n",
    "8. [Troubleshooting](#troubleshooting)\n\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n\n",
    "ChatQnA is a Retrieval-Augmented Generation (RAG) system that combines document retrieval with LLM inference. This tutorial provides a comprehensive guide for deploying ChatQnA using vLLM on AMD GPUs with ROCm support, and performing pipeline performance evaluation.\n\n",
    "### Key Features\n",
    "- **vLLM Integration**: LLM serving with optimized inference on AMD Instinct GPUs\n",
    "- **AMD GPU Support**: ROCm-based GPU acceleration\n",
    "- **Vector Search**: Redis-based document retrieval\n",
    "- **RAG Pipeline**: Complete question-answering system\n",
    "- **Performance Monitoring**: Built-in metrics and evaluation tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n\n",
    "- **AMD Developer Cloud**: 1xMI300X GPU / 192 GB VRAM / 20 vCPU / 240 GB RAM Droplet\n",
    "- **Hugging Face Token**: For model access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Platform for Enterprise AI (OPEA)\n",
    "!git clone https://github.com/opea-project/GenAIExamples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One click deployment scripts for the use case\n",
    "!git clone https://github.com/Yu-amd/LaunchPad.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LaunchPad project uses the same hierarchy as OPEA project. You need to copy the scripts and yaml files from the directory:  to the corresponding directory in OPEA folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy necessary scripts and configuration files to the OPEA directory\n",
    "# Replace /path/to/OPEA with your actual OPEA path\n",
    "!cp LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/*.sh /path/to/OPEA/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n",
    "!cp LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/*.yaml /path/to/OPEA/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the OPEA deployment directory\n",
    "%cd /path/to/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your Hugging Face token and environment\n",
    "# Edit the following line in set_env_vllm.sh with your Hugging Face Token\n",
    "!echo 'export CHATQNA_HUGGINGFACEHUB_API_TOKEN=\"your hugging face token\"' >> set_env_vllm.sh\n\n",
    "# Source the vLLM environment configuration\n",
    "!source set_env_vllm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Deploy Services\n\n",
    "#### Option A: Using the Unified Script (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup vLLM environment\n",
    "!./run_chatqna.sh setup-vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start vLLM services\n",
    "!./run_chatqna.sh start-vllm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service status\n",
    "!./run_chatqna.sh status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check chatqna-vllm-service status\n",
    "!docker logs -f chatqna-vllm-service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B: Manual Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source environment variables\n",
    "!source set_env_vllm.sh\n\n",
    "# Start all services\n",
    "!docker compose -f compose_vllm.yaml up -d\n\n",
    "# Check service status\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Verify Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check running containers\n",
    "!docker ps --format \"table {{.Names}}\\t{{.Status}}\\t{{.Ports}}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backend API\n",
    "!curl -X POST http://localhost:8890/v1/chatqna \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"Hello, how are you?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your public IP\n",
    "!hostname -I | awk '{print }'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the web interface at: \n\n",
    "### Step 5: Upload Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text file\n",
    "!echo \"Your document content here\" > document.txt\n\n",
    "# Upload the file\n",
    "!curl -X POST http://localhost:18104/v1/dataprep/ingest \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"files=@document.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the upload worked\n",
    "# Check if the document was indexed\n",
    "!curl -X POST http://localhost:18104/v1/dataprep/get \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"index_name\": \"rag-redis\"}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple documents\n",
    "# Create multiple files\n",
    "!echo \"Document 1 content\" > doc1.txt\n",
    "!echo \"Document 2 content\" > doc2.txt\n\n",
    "# Upload multiple files\n",
    "!curl -X POST http://localhost:18104/v1/dataprep/ingest \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"files=@doc1.txt\" \\\n",
    "  -F \"files=@doc2.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n\n",
    "### Overview\n\n",
    "Performance evaluation helps you understand:\n",
    "- **Throughput**: Requests per second\n",
    "- **Latency**: Response time\n",
    "- **Accuracy**: Answer quality\n",
    "- **Resource Usage**: CPU, GPU, memory utilization\n\n",
    "### Step 1: Setup Evaluation Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull from OPEA GitHub so GenAIExamples and GenAIEval are in the same directory\n",
    "!git clone https://github.com/opea-project/GenAIEval.git\n\n",
    "# Navigate to evaluation directory\n",
    "%cd /path/to/GenAIEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy chatqna scripts from the LaunchPad directory\n",
    "!cp /path/to/LaunchPad/GenAIEval/evals/benchmark/* /path/to/GenAIEval/evals/benchmark/\n\n",
    "# Install dependency\n",
    "!apt install python3.12-venv\n\n",
    "# Create virtual environment\n",
    "!python3 -m venv opea_eval_env\n",
    "!source opea_eval_env/bin/activate\n\n",
    "# Install evaluation dependencies\n",
    "!pip install -r requirements.txt\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run Basic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate back to GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n",
    "%cd /path/to/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n\n",
    "# Run vLLM evaluation\n",
    "!./run_chatqna.sh vllm-eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Performance Metrics\n\n",
    "#### Throughput Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependency\n",
    "!apt install apache2-utils\n\n",
    "# Create a complex test file\n",
    "!echo '{\"messages\": \"Can you provide a detailed explanation of how neural networks work, including the concepts of forward propagation, backpropagation, and gradient descent? Also explain how these concepts relate to deep learning and why they are important for modern AI systems.\"}' > test_data.json\n\n",
    "# Test concurrent requests\n",
    "!ab -n 100 -c 10 -p test_data.json -T application/json \\\n",
    "  http://localhost:8890/v1/chatqna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latency Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create curl-format.txt with the following content:\n",
    "curl_format_content = \"\"\"\n",
    "     time_namelookup:  %{time_namelookup}\\n\n",
    "        time_connect:  %{time_connect}\\n\n",
    "     time_appconnect:  %{time_appconnect}\\n\n",
    "    time_pretransfer:  %{time_pretransfer}\\n\n",
    "       time_redirect:  %{time_redirect}\\n\n",
    "  time_starttransfer:  %{time_starttransfer}\\n\n",
    "                     ----------\\n\n",
    "          time_total:  %{time_total}\\n\n",
    "          http_code:  %{http_code}\\n\n",
    "       size_download:  %{size_download}\\n\n",
    "      speed_download:  %{speed_download}\\n\n",
    "\"\"\"\n\n",
    "with open('curl-format.txt', 'w') as f:\n",
    "    f.write(curl_format_content)\n\n",
    "# Measure response times\n",
    "!curl -w \"@curl-format.txt\" -X POST http://localhost:8890/v1/chatqna \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"What is machine learning?\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Evaluation Results\n\n",
    "Evaluation results include:\n",
    "- **Response Time**: Average, median, 95th percentile\n",
    "- **Throughput**: Requests per second\n",
    "- **Accuracy**: Answer quality metrics\n",
    "- **Resource Usage**: CPU, GPU, memory consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring and Troubleshooting\n\n",
    "### Service Monitoring\n\n",
    "#### Check Service Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all services\n",
    "!./run_chatqna.sh status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check specific service logs\n",
    "!docker compose -f compose_vllm.yaml logs -f chatqna-vllm-service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitor Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy prometheus.yml from LaunchPad directory\n",
    "!cp /path/to/LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/grafana/prometheus.yml /path/to/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/grafana/\n\n",
    "# Start monitoring stack\n",
    "!./run_chatqna.sh monitor-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the grafana files from LaunchPad to GenAIExample directory\n",
    "!cp -r /path/to/LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/grafana/* /path/to/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/grafana/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access Grafana dashboard at:  (admin/admin)\n\n",
    "### Data Source Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prometheus Data Source\n",
    "!curl -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Basic YWRtaW46YWRtaW4=\" \\\n",
    "  -d '{\n",
    "    \"name\": \"Prometheus\",\n",
    "    \"type\": \"prometheus\",\n",
    "    \"url\": \"http://prometheus:9090\",\n",
    "    \"access\": \"proxy\",\n",
    "    \"isDefault\": true\n",
    "  }' \\\n",
    "  http://localhost:3000/api/datasources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dashboard Imports\n\n",
    "#### 1. Comprehensive Dashboard (TGI + vLLM) - Fixed\n",
    "**Use this for remote nodes with both TGI and vLLM services**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Basic YWRtaW46YWRtaW4=\" \\\n",
    "  -d @grafana/dashboards/chatqna_comprehensive_dashboard_vllm_fixed_import.json \\\n",
    "  http://localhost:3000/api/dashboards/import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. AI Model Performance Dashboard\n",
    "**Use this for detailed model-specific monitoring and performance analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Basic YWRtaW46YWRtaW4=\" \\\n",
    "  -d @grafana/dashboards/chatqna_ai_model_dashboard_import.json \\\n",
    "  http://localhost:3000/api/dashboards/import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. TGI-Only Dashboard (Local Development)\n",
    "**Use this for local development where vLLM is not available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Basic YWRtaW46YWRtaW4=\" \\\n",
    "  -d @grafana/dashboards/chatqna_tgi_only_dashboard_import.json \\\n",
    "  http://localhost:3000/api/dashboards/import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Issues and Solutions\n\n",
    "#### Issue 1: GPU Memory Errors\n",
    "**Symptoms**:  or similar errors\n",
    "**Solution**:\n",
    "\n\n",
    "#### Issue 2: Service Startup Failures\n",
    "**Symptoms**: Services fail to start or remain in \"starting\" state\n",
    "**Solution**:\n",
    "\n\n",
    "#### Issue 3: Redis Index Issues\n",
    "**Symptoms**: Retrieval service fails to find documents\n",
    "**Solution**:\n",
    "\n\n",
    "#### Issue 4: Model Download Failures\n",
    "**Symptoms**: Services fail to download models\n",
    "**Solution**:\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Configuration\n\n",
    "### Custom Model Configuration\n\n",
    "Edit  to use different models:\n\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n\n",
    "### Diagnostic Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system resources\n",
    "!./detect_issues.sh\n\n",
    "# Test complete system\n",
    "!./quick_test_chatqna.sh eval-only\n\n",
    "# Check service health\n",
    "!docker compose -f compose_vllm.yaml ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all logs\n",
    "!docker compose -f compose_vllm.yaml logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow specific service logs\n",
    "!docker compose -f compose_vllm.yaml logs -f chatqna-vllm-service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for errors\n",
    "!docker compose -f compose_vllm.yaml logs | grep -i error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n\n",
    "This tutorial provides a comprehensive guide for deploying ChatQnA with vLLM on AMD GPUs and performing detailed performance evaluation. The system offers:\n\n",
    "- **High Performance**: vLLM-optimized inference\n",
    "- **Scalability**: Docker-based microservices architecture\n",
    "- **Monitoring**: Built-in performance metrics\n",
    "- **Flexibility**: Configurable models and parameters\n\n",
    "For additional support or advanced configurations, refer to the project documentation or create issues in the repository.\n\n",
    "### Next Steps\n\n",
    "1. **Customize Models**: Experiment with different LLM and embedding models\n",
    "2. **Scale Deployment**: Add multiple GPU nodes for higher throughput\n",
    "3. **Optimize Performance**: Fine-tune vLLM parameters for your specific use case\n",
    "4. **Monitor Production**: Set up comprehensive monitoring for production deployments\n\n",
    "### Useful Commands Reference\n\n",
    "\n\n",
    "---\n\n",
    "*Note**: This tutorial assumes you have the necessary permissions and that all required software is installed. For production deployments, consider additional security measures and monitoring solutions.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}