{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatQnA vLLM Deployment and Performance Evaluation Tutorial\n",
    "\n",
    "## Table of Contents\n",
    "1. [Overview](#overview)\n",
    "2. [Prerequisites](#prerequisites)\n",
    "3. [System Architecture](#system-architecture)\n",
    "4. [Deployment Guide](#deployment-guide)\n",
    "5. [Performance Evaluation](#performance-evaluation)\n",
    "6. [Monitoring and Troubleshooting](#monitoring-and-troubleshooting)\n",
    "7. [Advanced Configuration](#advanced-configuration)\n",
    "8. [Troubleshooting](#troubleshooting)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "ChatQnA is a Retrieval-Augmented Generation (RAG) system that combines document retrieval with LLM inference. This tutorial provides a comprehensive guide for deploying ChatQnA using vLLM on AMD GPUs with ROCm support, and performing pipeline performance evaluation.\n",
    "\n",
    "### Key Features\n",
    "- **vLLM Integration**: LLM serving with optimized inference on AMD Instinct GPUs\n",
    "- **AMD GPU Support**: ROCm-based GPU acceleration\n",
    "- **Vector Search**: Redis-based document retrieval\n",
    "- **RAG Pipeline**: Complete question-answering system\n",
    "- **Performance Monitoring**: Built-in metrics and evaluation tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "- **AMD Developer Cloud**: 1xMI300X GPU / 192 GB VRAM / 20 vCPU / 240 GB RAM Droplet\n",
    "- **Hugging Face Token**: For model access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## System Architecture\n",
    "\n",
    "### Service Components\n",
    "\n",
    "The following is the complete system architecture diagram.\n",
    "\n",
    "**Architecture Overview:**\n",
    "```\n",
    "┌───────────────────────────────────────────────────────────────────────────────────┐\n",
    "│                               EXTERNAL ACCESS                                     │\n",
    "│                                                                                   │\n",
    "│   ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────────────────┐   │\n",
    "│   │   Web Browser   │    │   API Clients   │    │      Monitoring Tools       │   │\n",
    "│   │                 │    │                 │    │    (Grafana, Prometheus)    │   │\n",
    "│   └─────────────────┘    └─────────────────┘    └─────────────────────────────┘   │\n",
    "│           │                       │                           │                   │\n",
    "│           │                       │                           │                   │\n",
    "│           ▼                       ▼                           ▼                   │\n",
    "│   ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────────────────┐   │\n",
    "│   │   Nginx Proxy   │    │   Backend API   │    │        Redis Insight        │   │\n",
    "│   │   (Port 8081)   │    │   (Port 8890)   │    │         (Port 8002)         │   │\n",
    "│   └─────────────────┘    └─────────────────┘    └─────────────────────────────┘   │\n",
    "│           │                       │                           │                   │\n",
    "│           │                       │                           │                   │\n",
    "│           ▼                       ▼                           ▼                   │\n",
    "│   ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────────────────┐   │\n",
    "│   │   Frontend UI   │    │     Backend     │    │   Redis Vector Database     │   │\n",
    "│   │   (Port 5174)   │    │     Server      │    │         (Port 6380)         │   │\n",
    "│   │   (React App)   │    │    (FastAPI)    │    │      (Vector Storage)       │   │\n",
    "│   └─────────────────┘    └─────────────────┘    └─────────────────────────────┘   │\n",
    "│                                   │                           │                   │\n",
    "│                                   │                           │                   │\n",
    "│                                   ▼                           ▼                   │\n",
    "│  ┌─────────────────────────────────────────────────────────────────────────────┐  │\n",
    "│  │                             RAG PIPELINE                                    │  │\n",
    "│  │                                                                             │  │\n",
    "│  │  ┌───────────────────┐ ┌─────────────────────┐ ┌─────────────────────────┐  │  │\n",
    "│  │  │ Retriever Service │ │TEI Embedding Service│ │  TEI Reranking Service  │  │  │\n",
    "│  │  │                   │ │                     │ │                         │  │  │\n",
    "│  │  │   (Port 7001)     │ │    (Port 18091)     │ │      (Port 18809)       │  │  │\n",
    "│  │  │                   │ │                     │ │                         │  │  │\n",
    "│  │  │ • Vector Search   │ │ • Text Embedding    │ │ • Document Reranking    │  │  │\n",
    "│  │  │ • Similarity      │ │ • BGE Model         │ │ • Relevance Scoring     │  │  │\n",
    "│  │  │   Matching        │ │ • CPU Inference     │ │ • CPU Inference         │  │  │\n",
    "│  │  └───────────────────┘ └─────────────────────┘ └─────────────────────────┘  │  │\n",
    "│  │            │                      │                         │               │  │\n",
    "│  │            │                      │                         │               │  │\n",
    "│  │            ▼                      ▼                         ▼               │  │\n",
    "│  │  ┌───────────────────────────────────────────────────────────────────────┐  │  │\n",
    "│  │  │                           vLLM Service                                │  │  │\n",
    "│  │  │                           (Port 18009)                                │  │  │\n",
    "│  │  │                                                                       │  │  │\n",
    "│  │  │                  • High-Performance LLM Inference                     │  │  │\n",
    "│  │  │                  • AMD GPU Acceleration (ROCm)                        │  │  │\n",
    "│  │  │                  • Qwen2.5-7B-Instruct Model                          │  │  │\n",
    "│  │  │                  • Optimized for Throughput & Latency                 │  │  │\n",
    "│  │  │                  • Tensor Parallel Support                            │  │  │\n",
    "│  │  └───────────────────────────────────────────────────────────────────────┘  │  │\n",
    "│  └─────────────────────────────────────────────────────────────────────────────┘  │\n",
    "│                                      │                                            │\n",
    "│                                      │                                            │\n",
    "│                                      ▼                                            │\n",
    "│  ┌─────────────────────────────────────────────────────────────────────────────┐  │\n",
    "│  │                            DATA PIPELINE                                    │  │\n",
    "│  │                                                                             │  │\n",
    "│  │  ┌─────────────────┐    ┌─────────────────┐    ┌─────────────────────────┐  │  │\n",
    "│  │  │   Dataprep      │    │   Model Cache   │    │   Document Storage      │  │  │\n",
    "│  │  │   Service       │    │   (./data)      │    │   (Redis Vector DB)     │  │  │\n",
    "│  │  │   (Port 18104)  │    │                 │    │                         │  │  │\n",
    "│  │  │                 │    │ • Downloaded    │    │ • Vector Embeddings     │  │  │\n",
    "│  │  │ • Document      │    │   Models        │    │ • Metadata Index        │  │  │\n",
    "│  │  │   Processing    │    │ • Model Weights │    │ • Full-Text Search      │  │  │\n",
    "│  │  │ • Text          │    │ • Cache Storage │    │ • Similarity Search     │  │  │\n",
    "│  │  │   Extraction    │    │ • Shared Volume │    │ • Redis Stack           │  │  │\n",
    "│  │  └─────────────────┘    └─────────────────┘    └─────────────────────────┘  │  │\n",
    "│  └─────────────────────────────────────────────────────────────────────────────┘  │\n",
    "└───────────────────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "**Additional Services:**\n",
    "- **Dataprep Service** (Port 18104): Document processing and ingestion\n",
    "- **Redis Insight** (Port 8002): Database monitoring interface\n",
    "- **Model Cache** (./data): Shared volume for model storage\n",
    "\n",
    "### Data Flow\n",
    "1. **User Input**: Question submitted via frontend\n",
    "2. **Embedding**: Question converted to vector using TEI service\n",
    "3. **Retrieval**: Similar documents retrieved from Redis vector database\n",
    "4. **Reranking**: Retrieved documents reranked for relevance\n",
    "5. **LLM Inference**: vLLM generates answer using retrieved context\n",
    "6. **Response**: Answer returned to user via frontend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment Guide\n",
    "\n",
    "### Step 1: Pull source code from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open Platform for Enterprise AI (OPEA)\n",
    "!git clone https://github.com/opea-project/GenAIExamples.git"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One click deployment scripts for the use case\n",
    "!git clone https://github.com/Yu-amd/LaunchPad.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LaunchPad project uses the same hierarchy as OPEA project. You need to copy the scripts and yaml files from the directory:  to the corresponding directory in OPEA folder:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy necessary scripts and configuration files to the OPEA directory\n",
    "# Replace /path/to/OPEA repos with your actual OPEA path\n",
    "!cp LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/*.sh /path/to/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n",
    "!cp LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/*.yaml /path/to/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate to the OPEA deployment directory\n",
    "%cd /path/to/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up your Hugging Face token and environment\n",
    "# Edit the following line in set_env_vllm.sh with your Hugging Face Token\n",
    "export CHATQNA_HUGGINGFACEHUB_API_TOKEN=\"your hugging face token\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source the vLLM environment configuration\n",
    "!source set_env_vllm.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Deploy Services\n",
    "\n",
    "#### Option A: Using the Unified Script (Recommended)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup vLLM environment\n",
    "!./run_chatqna.sh setup-vllm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start vLLM services\n",
    "!./run_chatqna.sh start-vllm"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service status\n",
    "!./run_chatqna.sh status"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check chatqna-vllm-service status\n",
    "!docker logs -f chatqna-vllm-service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Option B: Manual Deployment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source environment variables\n",
    "!source set_env_vllm.sh"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start all services\n",
    "!docker compose -f compose_vllm.yaml up -d"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [   
    "# Check service status\n",
    "!docker ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU Memory Management\n",
    "\n",
    "Before proceeding to verify deployment, it's important to ensure your GPU memory is properly managed.\n",
    "\n",
    "#### Check GPU Memory Status\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current GPU memory usage\n",
    "# Expected output shows VRAM% and GPU% usage\n",
    "# If VRAM% is high (>80%) but GPU% is low, memory may be fragmented\n",
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clear GPU Memory (If Needed)\n",
    "\n",
    "If you encounter GPU memory issues or high VRAM usage with low GPU utilization:\n",
    "\n",
    "**Option 1: Kill GPU Processes**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find processes using GPU\n",
    "!sudo fuser -v /dev/kfd"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kill GPU-related processes\n",
    "!sudo pkill -f \"python|vllm|docker\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**Option 2: Restart GPU Services**\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart AMD GPU services\n",
    "!sudo systemctl restart amdgpu\n",
    "!sudo systemctl restart kfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 3: System Reboot (Most Reliable)**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If other methods don't work, reboot the system\n",
    "# Note: If you're on a remote server, wait approximately 30 seconds to 1 minute\n",
    "# before attempting to SSH back into the server\n",
    "!sudo reboot"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "After clearing GPU memory, verify it's free:\n",
    "# Check GPU memory is now available\n",
    "# Expected: VRAM% should be low (<20%) and GPU% should be 0%\n",
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Verify Deployment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check running containers\n",
    "!docker ps --format \"table {{.Names}}\\t{{.Status}}\\t{{.Ports}}\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test backend API\n",
    "!curl -X POST http://localhost:8890/v1/chatqna \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"Hello, how are you?\"}'"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your public IP\n",
    "!hostname -I | awk '{print $1}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access the web interface at: http://your_public_IP:5174\n",
    "### Step 5: Upload Documents"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a text file\n",
    "!echo \"Your document content here\" > document.txt\n",
    "\n",
    "# Upload the file\n",
    "!curl -X POST http://localhost:18104/v1/dataprep/ingest \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"files=@document.txt\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the upload worked\n",
    "# Check if the document was indexed\n",
    "!curl -X POST http://localhost:18104/v1/dataprep/get \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"index_name\": \"rag-redis\"}'"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For multiple documents\n",
    "# Create multiple files\n",
    "!echo \"Document 1 content\" > doc1.txt\n",
    "!echo \"Document 2 content\" > doc2.txt\n",
    "\n",
    "# Upload multiple files\n",
    "!curl -X POST http://localhost:18104/v1/dataprep/ingest \\\n",
    "  -H \"Content-Type: multipart/form-data\" \\\n",
    "  -F \"files=@doc1.txt\" \\\n",
    "  -F \"files=@doc2.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Evaluation\n",
    "\n",
    "### Overview\n",
    "\n",
    "Performance evaluation helps you understand:\n",
    "- **Throughput**: Requests per second\n",
    "- **Latency**: Response time\n",
    "- **Accuracy**: Answer quality\n",
    "- **Resource Usage**: CPU, GPU, memory utilization\n",
    "\n",
    "### Step 1: Setup Evaluation Environment"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull from OPEA GitHub so GenAIExamples and GenAIEval are in the same directory\n",
    "!git clone https://github.com/opea-project/GenAIEval.git"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [  
    "# Navigate to evaluation directory\n",
    "%cd /path/to/GenAIEval"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy chatqna scripts from the LaunchPad directory\n",
    "!cp /path/to/LaunchPad/GenAIEval/evals/benchmark/* /path/to/GenAIEval/evals/benchmark/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [ 
    "# Install dependency\n",
    "!apt install python3.12-venv"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create virtual environment\n",
    "!python3 -m venv opea_eval_env\n",
    "!source opea_eval_env/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install evaluation dependencies\n",
    "!pip install -r requirements.txt\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Run Basic Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Navigate back to GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/\n",
    "%cd /path/to/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run vLLM evaluation\n",
    "!./run_chatqna.sh vllm-eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Performance Metrics\n",
    "\n",
    "#### Throughput Testing"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependency\n",
    "!apt install apache2-utils"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a complex test file\n",
    "!echo '{\"messages\": \"Can you provide a detailed explanation of how neural networks work, including the concepts of forward propagation, backpropagation, and gradient descent? Also explain how these concepts relate to deep learning and why they are important for modern AI systems.\"}' > test_data.json"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test concurrent requests\n",
    "!ab -n 100 -c 10 -p test_data.json -T application/json \\\n",
    "  http://localhost:8890/v1/chatqna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Latency Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create curl-format.txt with the following content:\n",
    "     time_namelookup:  %{time_namelookup}\\n\n",
    "        time_connect:  %{time_connect}\\n\n",
    "     time_appconnect:  %{time_appconnect}\\n\n",
    "    time_pretransfer:  %{time_pretransfer}\\n\n",
    "       time_redirect:  %{time_redirect}\\n\n",
    "  time_starttransfer:  %{time_starttransfer}\\n\n",
    "                     ----------\\n\n",
    "          time_total:  %{time_total}\\n\n",
    "          http_code:  %{http_code}\\n\n",
    "       size_download:  %{size_download}\\n\n",
    "      speed_download:  %{speed_download}\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measure response times\n",
    "!curl -w \"@curl-format.txt\" -X POST http://localhost:8890/v1/chatqna \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\"messages\": \"What is machine learning?\"}'"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Step 4: Evaluation Results\n",
    "\n",
    "Evaluation results include:\n",
    "- **Response Time**: Average, median, 95th percentile\n",
    "- **Throughput**: Requests per second\n",
    "- **Accuracy**: Answer quality metrics\n",
    "- **Resource Usage**: CPU, GPU, memory consumption"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Monitoring and Troubleshooting\n",
    "\n",
    "### Service Monitoring\n",
    "\n",
    "#### Check Service Status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check all services\n",
    "!./run_chatqna.sh status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check specific service logs\n",
    "!docker compose -f compose_vllm.yaml logs -f chatqna-vllm-service"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#### Monitor Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy prometheus.yml from LaunchPad directory\n",
    "!cp /path/to/LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/grafana/prometheus.yml /path/to/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/grafana/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start monitoring stack\n",
    "!./run_chatqna.sh monitor-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the grafana files from LaunchPad to GenAIExample directory\n",
    "!cp -r /path/to/LaunchPad/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/grafana/* /path/to/GenAIExamples/ChatQnA/docker_compose/amd/gpu/rocm/grafana/"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "You can access Grafana dashboard at: (admin/admin)\n",
    "\n",
    "### Data Source Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prometheus Data Source\n",
    "!curl -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Basic YWRtaW46YWRtaW4=\" \\\n",
    "  -d '{\n",
    "    \"name\": \"Prometheus\",\n",
    "    \"type\": \"prometheus\",\n",
    "    \"url\": \"http://prometheus:9090\",\n",
    "    \"access\": \"proxy\",\n",
    "    \"isDefault\": true\n",
    "  }' \\\n",
    "  http://localhost:3000/api/datasources"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Dashboard Imports\n",
    "\n",
    "#### 1. Comprehensive Dashboard (TGI + vLLM)\n",
    "**Use this for remote nodes with both TGI and vLLM services**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Basic YWRtaW46YWRtaW4=\" \\\n",
    "  -d @grafana/dashboards/chatqna_comprehensive_dashboard_vllm_fixed_import.json \\\n",
    "  http://localhost:3000/api/dashboards/import"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#### 2. AI Model Performance Dashboard\n",
    "**Use this for detailed model-specific monitoring and performance analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Basic YWRtaW46YWRtaW4=\" \\\n",
    "  -d @grafana/dashboards/chatqna_ai_model_dashboard_import.json \\\n",
    "  http://localhost:3000/api/dashboards/import"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#### 3. TGI-Only Dashboard (Local Development)\n",
    "**Use this for local development where vLLM is not available**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -X POST \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -H \"Authorization: Basic YWRtaW46YWRtaW4=\" \\\n",
    "  -d @grafana/dashboards/chatqna_tgi_only_dashboard_import.json \\\n",
    "  http://localhost:3000/api/dashboards/import"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Common Issues and Solutions\n",
    "\n",
    "#### Issue 1: GPU Memory Errors\n",
    "**Symptoms**: out-of-memory or similar errors\n",
    "\n",
    "**Solution**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce batch size in vLLM configuratioin\n",
    "# Edit compose_vllm.yaml, modify vLLM service command:\n",
    "--max-model-len 2048 --tensor-parallel-size 1"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [ 
    "#### Issue 2: Service Startup Failures\n",
    "**Symptoms**: Services fail to start or remain in \"starting\" state\n",
    "\n",
    "**Solution**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check logs for specific errors\n",
    "!docker compose -f compose_vllm.yaml logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart services\n",
    "!./run_chatqna.sh restart-vllm"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {}, 
   "source": [
    "#### Issue 3: Redis Index Issues\n",
    "**Symptoms**: Retrieval service fails to find documents\n",
    "\n",
    "**Solution**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix Redis index\n",
    "./fix_redis_index.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate index manually\n",
    "!docker exec chatqna-redis-vector-db redis-cli FT.CREATE rag-redis ON HASH PREFIX 1 doc: SCHEMA content TEXT WEIGHT 1.0 distance NUMERIC"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "#### Issue 4: Model Download Failures\n",
    "**Symptoms**: Services fail to download models\n",
    "\n",
    "**Solution**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check HF token\n",
    "!echo $CHATQNA_HUGGINGFACEHUB_API_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set token manually\n",
    "!export CHATQNA_HUGGINGFACEHUB_API_TOKEN=\"your_token_here\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Advanced Configuration\n",
    "\n",
    "### Custom Model Configuration\n",
    "\n",
    "Edit set_env_vllm.sh to use different models:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change LLM model\n",
    "!export CHATQNA_LLM_MODEL_ID=\"Qwen/Qwen2.5-14B-Instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change embedding model\n",
    "!export CHATQNA_EMBEDDING_MODEL_ID=\"BAAI/bge-large-en-v1.5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change reranking model\n",
    "!export CHATQNA_RERANK_MODEL_ID=\"BAAI/bge-reranker-large\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Diagnostic Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check system resources\n",
    "!./detect_issues.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complete system\n",
    "!./quick_test_chatqna.sh eval-only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check service health\n",
    "!docker compose -f compose_vllm.yaml ps"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "### Log Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all logs\n",
    "!docker compose -f compose_vllm.yaml logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow specific service logs\n",
    "!docker compose -f compose_vllm.yaml logs -f chatqna-vllm-service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for errors\n",
    "!docker compose -f compose_vllm.yaml logs | grep -i error"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This tutorial provides a comprehensive guide for deploying ChatQnA with vLLM on AMD GPUs and performing detailed performance evaluation. The system offers:\n",
    "\n",
    "- **High Performance**: vLLM-optimized inference\n",
    "- **Scalability**: Docker-based microservices architecture\n",
    "- **Monitoring**: Built-in performance metrics\n",
    "- **Flexibility**: Configurable models and parameters\n",
    "\n",
    "For additional support or advanced configurations, refer to the project documentation or create issues in the repository.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. **Customize Models**: Experiment with different LLM and embedding models\n",
    "2. **Scale Deployment**: Add multiple GPU nodes for higher throughput\n",
    "3. **Optimize Performance**: Fine-tune vLLM parameters for your specific use case\n",
    "4. **Monitor Production**: Set up comprehensive monitoring for production deployments\n",
    "\n",
    "### Useful Commands Reference\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "*Note**: This tutorial assumes you have the necessary permissions and that all required software is installed. For production deployments, consider additional security measures and monitoring solutions.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
